{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APUNTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow versión: 2.19.0\n",
      "Keras versión: 3.9.1\n",
      "Módulo keras.preprocessing importado correctamente.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "\n",
    "print(\"TensorFlow versión:\", tf.__version__)\n",
    "print(\"Keras versión:\", tf.keras.__version__)\n",
    "\n",
    "# Intentar importar los módulos\n",
    "from tensorflow.keras.preprocessing import image\n",
    "print(\"Módulo keras.preprocessing importado correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                filename  category\n",
      "11893  c:\\Users\\mamen\\OneDrive\\Documentos\\GitHub\\deep...         1\n",
      "23384  c:\\Users\\mamen\\OneDrive\\Documentos\\GitHub\\deep...         0\n",
      "3919   c:\\Users\\mamen\\OneDrive\\Documentos\\GitHub\\deep...         1\n",
      "19096  c:\\Users\\mamen\\OneDrive\\Documentos\\GitHub\\deep...         0\n",
      "218    c:\\Users\\mamen\\OneDrive\\Documentos\\GitHub\\deep...         1\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(dogs_path, cats_path, val_size=0.15, test_size=0.15, random_state=42):\n",
    "    # Obtener listas de archivos en cada carpeta\n",
    "    files_dogs = os.listdir(dogs_path)\n",
    "    files_cats = os.listdir(cats_path)\n",
    "\n",
    "    # Crear DataFrame con categorías\n",
    "    df_dogs = pd.DataFrame({'filename': files_dogs, 'category': 1})  # 1 para perros\n",
    "    df_cats = pd.DataFrame({'filename': files_cats, 'category': 0})  # 0 para gatos\n",
    "\n",
    "    # Agregar prefijos para indicar las rutas completas\n",
    "    df_dogs[\"filename\"] = df_dogs[\"filename\"].apply(lambda x: os.path.join(dogs_path, x))\n",
    "    df_cats[\"filename\"] = df_cats[\"filename\"].apply(lambda x: os.path.join(cats_path, x))\n",
    "\n",
    "    # Unir ambos DataFrames\n",
    "    df = pd.concat([df_dogs, df_cats], ignore_index=True)\n",
    "\n",
    "    # Dividir en entrenamiento, validación y prueba\n",
    "    train_df, temp_df = train_test_split(df, test_size=(val_size + test_size), stratify=df[\"category\"], random_state=random_state)\n",
    "    val_ratio = val_size / (val_size + test_size)  \n",
    "    val_df, test_df = train_test_split(temp_df, test_size=(1 - val_ratio), stratify=temp_df[\"category\"], random_state=random_state)\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Definir rutas de datos\n",
    "current_dir = os.getcwd()\n",
    "files_path_train = os.path.join(current_dir, \"../data/raw/train\")\n",
    "\n",
    "dogs_path = os.path.join(files_path_train, \"dogs\")\n",
    "cats_path = os.path.join(files_path_train, \"cats\")\n",
    "\n",
    "# Llamar a la función\n",
    "train_df, val_df, test_df = prepare_data(dogs_path, cats_path)\n",
    "\n",
    "# Mostrar algunas filas para verificar\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Images: 17500\n",
      "Total Validation Images: 3750\n",
      "Total Test Images: 3750\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Training Images: {len(train_df)}\")\n",
    "print(f\"Total Validation Images: {len(val_df)}\")\n",
    "print(f\"Total Test Images: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def load_and_process_images(dataframe, img_size=(200, 200)):\n",
    "#     X = []  # Lista para imágenes\n",
    "#     y = []  # Lista para etiquetas\n",
    "    \n",
    "#     for index, row in dataframe.iterrows():\n",
    "#         img_path = row[\"filename\"]  # Ruta completa de la imagen\n",
    "#         label = row[\"category\"]  # Etiqueta (1 = perro, 0 = gato)\n",
    "        \n",
    "#         # Cargar imagen y redimensionar\n",
    "#         img = load_img(img_path, target_size=img_size)  \n",
    "#         img_array = img_to_array(img) / 255.0  # Normalizar valores entre 0 y 1\n",
    "        \n",
    "#         # Guardar en listas\n",
    "#         X.append(img_array)\n",
    "#         y.append(label)\n",
    "\n",
    "#     # Convertir listas a arrays de NumPy\n",
    "#     X = np.array(X, dtype=np.float32)\n",
    "#     y = np.array(y, dtype=np.int32)\n",
    "\n",
    "#     return X, y\n",
    "\n",
    "# # Cargar imágenes del conjunto de entrenamiento\n",
    "# X_train, y_train = load_and_process_images(train_df)\n",
    "\n",
    "# # Mostrar forma de los datos\n",
    "# print(\"Tamaño de X_train:\", X_train.shape)  # Esperado: (25000, 200, 200, 3)\n",
    "# print(\"Tamaño de y_train:\", y_train.shape)  # Esperado: (25000,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_path = os.path.join(current_dir, \"../data/processed\")\n",
    "# save_train_path = os.path.join(processed_path, \"train_data.npz\")\n",
    "# np.savez(save_train_path, X_train=X_train, y_train=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si tengo guardado en el archivo, lo extraigo de save_train_path\n",
    "# # 📌 Cargar el archivo .npz\n",
    "# data = np.load(\"train_data.npz\")\n",
    "\n",
    "# # 📌 Asignar los datos a variables\n",
    "# X_train = data[\"X_train\"]\n",
    "# y_train = data[\"y_train\"]\n",
    "\n",
    "# # 📌 Verificar dimensiones\n",
    "# print(\"X_train shape:\", X_train.shape)\n",
    "# print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 2 classes.\n",
      "Found 5000 images belonging to 2 classes.\n",
      "Found 12500 validated image filenames.\n",
      "Clases detectadas: {'cats': 0, 'dogs': 1}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Directorios de imágenes\n",
    "train_dir = os.path.join(current_dir, \"../data/raw/train\")\n",
    "test_dir = os.path.join(current_dir, \"../data/raw/test1/unknown\")\n",
    "\n",
    "# 📌 Crear objeto ImageDataGenerator para AUMENTACIÓN y normalización\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,  # Normaliza valores entre 0-1\n",
    "    rotation_range=20,   # Rotación aleatoria\n",
    "    width_shift_range=0.2,  # Desplazamiento horizontal\n",
    "    height_shift_range=0.2, # Desplazamiento vertical\n",
    "    shear_range=0.2,  # Transformación de corte\n",
    "    zoom_range=0.2,   # Zoom aleatorio\n",
    "    horizontal_flip=True,  # Volteo horizontal\n",
    "    fill_mode=\"nearest\",  # Rellenar huecos con valores cercanos\n",
    "    validation_split=0.2  # 🔹 20% de los datos serán de validación\n",
    ")\n",
    "\n",
    "# 📌 Crear lista de archivos\n",
    "test_files = os.listdir(test_dir)\n",
    "\n",
    "# 📌 Crear un DataFrame (sin etiquetas, ya que son desconocidas)\n",
    "test_df = pd.DataFrame({'filename': test_files})\n",
    "\n",
    "# 📌 Crear un ImageDataGenerator (solo normalización)\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "# 📌 Cargar imágenes de entrenamiento con etiquetas AUTOMÁTICAS\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,  \n",
    "    target_size=(200, 200),  # Redimensiona las imágenes\n",
    "    batch_size=32,  # Número de imágenes por lote\n",
    "    class_mode=\"binary\",  # Etiquetas binarias (0 = gato, 1 = perro)\n",
    "    subset=\"training\"  # 🔹 Usa 80% para entrenamiento\n",
    ")\n",
    "# 📌 Generador de validación\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,  \n",
    "    target_size=(200, 200),\n",
    "    batch_size=32,\n",
    "    class_mode=\"binary\",\n",
    "    subset=\"validation\"  # 🔹 Usa 20% para validación\n",
    ")\n",
    "\n",
    "# 📌 Cargar imágenes del test (sin etiquetas)\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    test_df,  # DataFrame con los archivos\n",
    "    directory=test_dir,  # Ruta donde están las imágenes\n",
    "    x_col=\"filename\",  # Nombre de la columna con los archivos\n",
    "    target_size=(224, 224),  # Redimensionar imágenes\n",
    "    batch_size=32,\n",
    "    class_mode=None,  # ⚠️ No hay etiquetas, solo cargamos imágenes\n",
    "    shuffle=False  # Mantener el orden original\n",
    ")\n",
    "\n",
    "# 📌 Mostrar clases detectadas\n",
    "print(\"Clases detectadas:\", train_generator.class_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mamen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses\\losses.py:33: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m102/625\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:30:42\u001b[0m 31s/step - accuracy: 0.5014 - loss: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 🔹 1. Definir el modelo basado en la arquitectura dada\n",
    "model = Sequential()\n",
    "model.add(Conv2D(input_shape=(200, 200, 3), filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=4096, activation=\"relu\"))\n",
    "model.add(Dense(units=4096, activation=\"relu\"))\n",
    "model.add(Dense(units=1, activation=\"sigmoid\"))  # 2 clases: perros y gatos. 1 neurona sigmoid\n",
    "\n",
    "# 🔹 2. Compilar el modelo\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),  # Optimizador Adam con tasa de aprendizaje baja\n",
    "              loss=\"categorical_crossentropy\",  # Pérdida para clasificación multiclase\n",
    "              metrics=[\"accuracy\"])  # Medir precisión\n",
    "\n",
    "# 🔹 3. Definir un Early Stopping para evitar sobreajuste\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "# 🔹 4. Entrenar el modelo\n",
    "history = model.fit(\n",
    "    train_generator,  # Datos de entrenamiento\n",
    "    epochs=5,  # Número de épocas recomendable 20\n",
    "    batch_size=32,  \n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# 🔹 5. Evaluar rendimiento en datos de prueba\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"\\n🔹 Precisión en prueba: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📌 Hacer predicciones\n",
    "predictions = model.predict(test_generator)\n",
    "\n",
    "# 📌 Convertir predicciones a etiquetas (0 = gato, 1 = perro)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# 📌 Agregar etiquetas al DataFrame\n",
    "test_df[\"label\"] = predicted_labels\n",
    "test_df[\"label\"] = test_df[\"label\"].map({0: \"cat\", 1: \"dog\"})  # Convertir 0/1 a texto\n",
    "\n",
    "# 📌 Mostrar los primeros resultados\n",
    "print(test_df.head())\n",
    "\n",
    "# 📌 Guardar resultados en CSV\n",
    "test_df.to_csv(\"test_predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
